{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from mmdetection.mmdet.apis import init_detector, inference_detector, show_result\n",
    "from tqdm import tqdm_notebook\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "prediction_file_path = 'preds.txt'\n",
    "ground_truth_file_path = 'ground-truth.txt'\n",
    "tile_json = json.load(open('../../data/ktr-test/tiles.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dict = {}\n",
    "\n",
    "for i in tile_json.keys():\n",
    "    frame = i.split('-')[0]\n",
    "    if frame in frames_dict:\n",
    "        frames_dict[frame].append(i)\n",
    "    else:\n",
    "        frames_dict[frame] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# config_file = '../mmdetection/configs/faster_rcnn_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../mmdetection/work_dirs/old_workdir/faster_rcnn_r50_fpn_1x_visdrone/epoch_1.pth'\n",
    "\n",
    "# config_file = '../mmdetection/configs/faster_rcnn_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../mmdetection/work_dirs/faster_rcnn_r50_fpn_1x_mix/epoch_4.pth'\n",
    "\n",
    "# config_file = '../mmdetection/configs/faster_rcnn_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../mmdetection/work_dirs/old_workdir/faster_rcnn_r50_fpn_1x_visdrone_pretrained/epoch_14.pth'\n",
    "\n",
    "# config_file = '../../src/configs/libra_rcnn/libra_retinanet_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../mmdetection/work_dirs/libra_retinanet_r50_fpn_1x/epoch_2.pth'\n",
    "\n",
    "# config_file = '../configs/guided_anchoring/ga_retinanet_x101_32x4d_fpn_1x.py'\n",
    "# checkpoint_file = '../../models/work_dirs/ga_retinanet_x101_32x4d_fpn_1x/latest.pth'\n",
    "\n",
    "\n",
    "# config_file = '../mmdetection/configs/retinanet_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../mmdetection/work_dirs/retinanet_r50_fpn_1x_visdrone/epoch_2.pth'\n",
    "\n",
    "# config_file = '../../src/configs/cascade_rcnn_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../../models/work_dirs/cascade_rcnn_r50_fpn_1x-all/latest.pth'\n",
    "\n",
    "\n",
    "# config_file = '../../models/google-cloud-models/cascade-t3-vis/config.py'\n",
    "# checkpoint_file = '../../models/google-cloud-models/cascade-t3-vis/epoch_1.pth'\n",
    "\n",
    "# config_file = '../../src/configs/cascade_rcnn_r50_fpn_1x.py'\n",
    "# checkpoint_file = '../../models/work_dirs/cascade_rcnn_r50_fpn_1x-cropped-t3only-nobp-yaya/latest.pth'\n",
    "\n",
    "config_file = '../mmdetection/configs/faster_rcnn_r50_fpn_1x.py'\n",
    "checkpoint_file = '../../models/work_dirs/faster_rcnn_r50_fpn_grid-yaya-t3-subset-visdrone/epoch_29.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0151adae7394e44b7927963ff2af7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65235), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "img_list = glob('../../data/ktr-test/test-grid/*/**.jpg')\n",
    "\n",
    "for img in tqdm_notebook(img_list):\n",
    "    results['/'.join(img.split('/')[-2:])[:-4]] = inference_detector(model, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c578cf1558b44e39e4e0320fb05fe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4349), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frame_bboxes = {}\n",
    "for k, v in tqdm_notebook(frames_dict.items()):\n",
    "    frame_bboxes[k] = []\n",
    "    for vv in v:\n",
    "        bboxes = results[vv][0]\n",
    "        x, y, _, _ = tile_json[vv]\n",
    "        for bbox in bboxes:\n",
    "            frame_bboxes[k].append(bbox + [x, y, x, y, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, probs=None, overlapThresh=0.6):\n",
    "    \n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes are integers, convert them to floats -- this\n",
    "    # is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # compute the area of the bounding boxes and grab the indexes to sort\n",
    "    # (in the case that no probabilities are provided, simply sort on the\n",
    "    # bottom-left y-coordinate)\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = y2\n",
    "\n",
    "    # if probabilities are provided, sort on them instead\n",
    "    if probs is not None:\n",
    "        idxs = probs\n",
    "\n",
    "    # sort the indexes\n",
    "    idxs = np.argsort(idxs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the index value\n",
    "        # to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of the bounding\n",
    "        # box and the smallest (x, y) coordinates for the end of the bounding\n",
    "        # box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have overlap greater\n",
    "        # than the provided overlap threshold\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return np.concatenate((boxes[pick].astype(\"int\"), np.expand_dims(probs[pick], -1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_frame_bboxes = {}\n",
    "for frame, bboxes in frame_bboxes.items():\n",
    "    if len(bboxes) > 0:\n",
    "        boxes, probs = np.array(bboxes)[:,:4], np.array(bboxes)[:,-1]\n",
    "        nms_frame_bboxes[frame] = non_max_suppression_fast(boxes, probs)\n",
    "    else:\n",
    "        nms_frame_bboxes[frame] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/deep/t3/data/ktr-data/test/B23072019_V1_K1/frame1.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(image_folder_base, list(frame_and_boxes.keys())[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B23072019_V1_K1/frame1004', [])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_path , frame_and_boxes[frame_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d8746f20694c4cb8e3da9d5587fba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4349), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "frame_and_boxes = nms_frame_bboxes\n",
    "\n",
    "\n",
    "# Input image base folder \n",
    "image_folder_base = '/home/deep/t3/data/ktr-test/test/'\n",
    "# Output video base folder\n",
    "video_output_base = './'\n",
    "\n",
    "\n",
    "video_name = '{}{}.mp4'.format(video_output_base, 'faster_rcnn_ktr')\n",
    "# sort images by frame\n",
    "\n",
    "results = []\n",
    "\n",
    "# get h,w to initialize canvas frame\n",
    "frame = cv2.imread(os.path.join(image_folder_base, list(frame_and_boxes.keys())[0]+'.jpg'))\n",
    "\n",
    "# resize\n",
    "height, width, layers = frame.shape\n",
    "height //= 2 \n",
    "width //= 2\n",
    "# arguments: output_name, codec, fps, size\n",
    "video = cv2.VideoWriter(video_name, 0x7634706d, 5, (width,height)) \n",
    "\n",
    "for frame_path in tqdm(frame_and_boxes.keys(), total=len(frame_and_boxes)):\n",
    "\n",
    "\n",
    "    img_frame = cv2.imread(os.path.join(image_folder_base, frame_path + '.jpg'))\n",
    "\n",
    "    res = np.array(frame_and_boxes[frame_path])\n",
    "    if len(res):\n",
    "        for bbox, prob in zip(res[:,:-1], res[:,-1]):\n",
    "            if prob > 0.6:\n",
    "                mins = int(bbox[0]), int(bbox[1])\n",
    "                maxs = int(bbox[2]), int(bbox[3])\n",
    "                img_frame = cv2.rectangle(img_frame, mins, maxs,(0, 0, 250),2)            \n",
    "                txt = str(prob)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                ((txt_w, txt_h), _) = cv2.getTextSize(txt, font, 0.35, 1)\n",
    "                # Place text background.\n",
    "                back_tl = bbox[0], bbox[1] - int(1.3 * txt_h)\n",
    "                back_br = bbox[0] + txt_w, bbox[1]\n",
    "                # Show text.\n",
    "                txt_tl = int(bbox[0]), int(bbox[1] - int(0.3 * txt_h))\n",
    "                cv2.putText(img_frame, f'{prob:.3f}', txt_tl, font, 0.6, (218, 227, 218), lineType=cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    bottomLeftCornerOfText = (10,500)\n",
    "    fontScale              = 1\n",
    "    fontColor              = (218, 227, 218)\n",
    "    lineType               = 2\n",
    "\n",
    "    cv2.putText(img_frame,str(frame_path),\n",
    "        bottomLeftCornerOfText, \n",
    "        font,\n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "    \n",
    "#     plt.rcParams[\"figure.figsize\"] = (20,14)\n",
    "#     plt.imshow(img_frame)\n",
    "#     break\n",
    "    \n",
    "    \n",
    "    video.write(cv2.resize(img_frame, (width, height)))\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(frame_bboxes, open('faster_rcnn_ktr_preds.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(nms_frame_bboxes, open('faster_rcnn_ktr_nms_preds.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_bboxes = pickle.load(open('faster_rcnn_ktr_preds.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3.7 MMDetect",
   "language": "python",
   "name": "mmdetect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
